{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests \n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import requests\n",
    "import datetime\n",
    "from langdetect import detect\n",
    "from htmldate import find_date\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "prefs={\"profile.managed_default_content_settings.images\": 2, 'disk-cache-size': 4096 }\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "path= r\"C:\\Users\\IKNOOR\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "user_agent_list = ['Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:76.0) Gecko/20100101 Firefox/76.0','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.4 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:76.0) Gecko/20100101 Firefox/76.0','Mozilla/5.0 (Windows NT 10.0; rv:68.0) Gecko/20100101 Firefox/68.0','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18362','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.18363','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36 OPR/67.0.3575.137','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36 Edg/81.0.416.64','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36 Edg/81.0.416.68','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36 OPR/68.0.3618.63','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36 Edg/81.0.416.72','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36 OPR/68.0.3618.104','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:56.0) Gecko/20100101 Firefox/56.0 Waterfox/56.2.14','Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0','Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0','Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 YaBrowser/20.3.2.242 Yowser/2.5 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36 OPR/68.0.3618.63','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:76.0) Gecko/20100101 Firefox/76.0','Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0','Mozilla/5.0 (X11; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (X11; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0','Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0','Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Set the date after which we need to scrape\n",
    "after = '2020/01/01'\n",
    "\n",
    "#Set number of pages to scrape from https://www.poynter.org/ifcn-covid-19-misinformation/\n",
    "num_pages = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collector=[]\n",
    "for i in range(1,num_pages+1):\n",
    "    url = 'https://www.poynter.org/ifcn-covid-19-misinformation/page/' + str(i) + '/?orderby=views&order=DSC#038;order=ASC'\n",
    "    print(i, url)\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup1 = BeautifulSoup(response.text, 'html.parser')\n",
    "    lis = soup1.find_all('div', attrs={'class':'post-container'})\n",
    "    link = [item.find('a', attrs={'class':'button entry-content__button entry-content__button--smaller'})['href'] for item in lis if item.find_all('p')[1].get_text()[0:10] > after]\n",
    "    if link == []:\n",
    "        break\n",
    "    print(\"New Links: \", len(link))\n",
    "    collector = collector + link\n",
    "    \n",
    "print(\"Total links collected: \", len(collector))\n",
    "links= list(set(collector))\n",
    "print(\"Total unique links collected: \", len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res=[]\n",
    "for i in range(len(links)):\n",
    "    fd = {}\n",
    "    print(i,links[i])\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(links[i],headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    fd['Link']=links[i]\n",
    "    fd['Factcheck_Org'] = soup.find('p', attrs={'class':'entry-content__text entry-content__text--org'}).get_text()[17:]\n",
    "    fd['Date'] = soup.find('p', attrs={'class':'entry-content__text entry-content__text--topinfo'}).get_text()[0:10]\n",
    "    fd['Country'] = soup.find('p', attrs={'class':'entry-content__text entry-content__text--topinfo'}).get_text()[13:]\n",
    "    fd['Label'] = soup.find('span', attrs={'class':'entry-title--red'}).get_text()[:-1]\n",
    "    ct = soup.find('h1', attrs={'class':'entry-title'})\n",
    "    for tag in ct.find_all('span'):\n",
    "        tag.replaceWith('')\n",
    "    fd['Claim'] = ct.get_text()\n",
    "    fd['Explaination'] = soup.find('p', attrs={'class':'entry-content__text entry-content__text--explanation'}).get_text()[13:]\n",
    "    fd['Source'] = soup.find('a', attrs={'class':'button entry-content__button entry-content__button--smaller'})['href']\n",
    "    fd['Origin'] = soup.find('p', attrs={'class':'entry-content__text entry-content__text--smaller'}).get_text().split(':')[1]\n",
    "    res.append(fd)\n",
    "\n",
    "print(\"Crawled IFCN data from each of the \", len(links), \" unique links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates based on claim+source   \n",
    "\n",
    "res_n = []\n",
    "uq = []\n",
    "for i in range(len(res)):\n",
    "    if res[i]['Claim']+res[i]['Source'] not in uq:\n",
    "        res_n.append(res[i])\n",
    "        uq.append(res[i]['Claim']+res[i]['Source'])\n",
    "res=res_n.copy()\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uri_validator(x):\n",
    "    try:\n",
    "        result = urlparse(x)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using selenium to crawl data from debunk page\n",
    "\n",
    "for i in range(len(res)):\n",
    "    print(i)\n",
    "    html=res[i]['Source']\n",
    "    if html==\"\" or html==\"#\":\n",
    "        res[i]['Debunk_Html'] = \"\"\n",
    "        res[i]['Source_PageTextOriginal'] = \"\"\n",
    "        res[i]['p_tag'] = \"\"\n",
    "        res[i]['Status_Code'] = None\n",
    "        continue\n",
    "    if not re.match('(?:http|ftp|https)://', html):\n",
    "        res[i]['Source'] = 'https://{}'.format(html)\n",
    "        html = res[i]['Source']\n",
    "    if not uri_validator(html):\n",
    "        res[i]['Debunk_Html'] = \"\"\n",
    "        res[i]['Source_PageTextOriginal'] = \"\"\n",
    "        res[i]['Status_Code'] = None\n",
    "        res[i]['p_tag'] = \"\"\n",
    "        continue\n",
    "        \n",
    "    try:    \n",
    "        neu = requests.head(html, allow_redirects=True).url\n",
    "    except Exception as e:\n",
    "        print(\"Error in redirects: \",e)\n",
    "        neu=html\n",
    "    if neu != html:\n",
    "        html = neu\n",
    "        res[i]['Source'] = neu\n",
    "        \n",
    "    if res[i]['Source'][0:5] == 'http:':\n",
    "        res[i]['Source'] = 'https:'+res[i]['Source'][5:] \n",
    "        html = res[i]['Source']\n",
    "        \n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    try:\n",
    "        response = requests.get(html, headers=headers)\n",
    "        status = response.status_code\n",
    "        res[i]['Status_Code'] = status\n",
    "    except Exception as e:\n",
    "        print(\"Error in requests: \",e)\n",
    "        res[i]['Status_Code'] = 400\n",
    "        res[i]['Debunk_Html'] = \"\"\n",
    "        res[i]['Source_PageTextOriginal'] = \"\"\n",
    "        res[i]['p_tag'] = \"\"\n",
    "        continue\n",
    "    \n",
    "    if res[i]['Status_Code'] == 400:\n",
    "        print(\"400 Error\")\n",
    "        res[i]['Debunk_Html'] = \"\"\n",
    "        res[i]['Source_PageTextOriginal'] = \"\"\n",
    "        res[i]['p_tag'] = \"\"\n",
    "        continue\n",
    "        \n",
    "    driver = webdriver.Chrome(path,chrome_options=chrome_options)\n",
    "    driver.get(html)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    lis = soup.find_all('p')\n",
    "    art = \"\"\n",
    "    for j in range(len(lis)):\n",
    "        line = lis[j].getText() + '\\n'\n",
    "        art = art + line    \n",
    "    res[i]['p_tag'] = art\n",
    "    \n",
    "    [s.extract() for s in soup(['style', 'noscript', 'path', 'symbol'])]\n",
    "    res[i]['Debunk_Html'] = str(soup)\n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title','meta', 'noscript','link', 'path', 'symbol'])]\n",
    "    visible_text = soup.getText()\n",
    "    res[i]['Source_PageTextOriginal'] = visible_text.strip()\n",
    "\n",
    "print(\"Crawled debunk links for each IFCN page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language used on each debunk page\n",
    "\n",
    "for i in range(len(res)):\n",
    "    if \"washingtonpost.com\" in res[i]['Source']:\n",
    "        temp = \"en\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"myanmar.factcrescendo.com\" in res[i]['Source']:\n",
    "        temp = \"my\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"srilanka.factcrescendo.com\" in res[i]['Source']:\n",
    "        temp = \"si\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"boommyanmar.com\" in res[i]['Source']:\n",
    "        temp = \"my\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"odia.factcrescendo\" in res[i]['Source']:\n",
    "        temp = \"or\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"assamese.factcrescendo\" in res[i]['Source']:\n",
    "        temp = \"as\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "    if \"factcheck.ge/ka\" in res[i]['Source']:\n",
    "        temp = \"ka\"\n",
    "        res[i]['Source_Lang']=temp\n",
    "        continue\n",
    "        \n",
    "    art = res[i]['Source_PageTextOriginal']\n",
    " \n",
    "    if art==\"\":\n",
    "        print(\"Empty Source Page Text found, setting to NA\")\n",
    "        res[i]['Source_Lang']=\"NA\"\n",
    "        continue   \n",
    "    try:\n",
    "        temp = detect(art)\n",
    "    except Exception as e:\n",
    "        print(\"Error in Langdetect: \", e)\n",
    "        print(\"Setting Source_Lang to NA\")\n",
    "        temp = \"NA\"\n",
    "        \n",
    "    res[i]['Source_Lang']=temp\n",
    "\n",
    "print(\"Added language id for each debunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today() - datetime.timedelta(days=1)\n",
    "datechk = today.strftime(\"%Y/%m/%d\") \n",
    "for i in range(len(res)):\n",
    "    try:\n",
    "        res[i]['Debunk_Date'] = find_date(res[i]['Debunk_Html'], outputformat='%Y/%m/%d', original_date=True)\n",
    "    except:\n",
    "        print(\"Error using library, old Date used\")\n",
    "        res[i]['Debunk_Date'] = res[i]['Date']\n",
    "    if res[i]['Debunk_Date']==\"\" or res[i]['Debunk_Date']==None or res[i]['Debunk_Date']<\"2020/01/14\" or res[i]['Debunk_Date']>=datechk or 'politifact' in res[i]['Source'] or 'chequeado.com' in res[i]['Source'] or 'rebaltica' in res[i]['Source'] or 'boliviaverifica' in res[i]['Source']:\n",
    "        res[i]['Debunk_Date']=res[i]['Date']\n",
    "print(\"Debunk date added\")\n",
    "for i in range(len(res)):\n",
    "    soup = BeautifulSoup(res[i]['Debunk_Html'], 'html.parser')\n",
    "    [s.extract() for s in soup(['style', 'script', 'noscript','link', 'path', 'symbol'])]\n",
    "    res[i]['Debunk_Html'] = str(soup)\n",
    "print(\"Cleaned HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Translating debunk page text to English\n",
    "\n",
    "for i in range(len(res)):\n",
    "    print(i)\n",
    "    if res[i]['Source_Lang'] == 'NA':\n",
    "        try:\n",
    "            blob = TextBlob(res[i]['Source_PageTextOriginal'])\n",
    "            trans_text = str(blob.translate(to='en'))\n",
    "            res[i]['Source_PageTextEnglish'] = trans_text\n",
    "            blob = TextBlob(res[i]['p_tag'])\n",
    "            trans_text = str(blob.translate(to='en'))\n",
    "            res[i]['p_tag'] = trans_text\n",
    "        except:\n",
    "            res[i]['Source_PageTextEnglish'] = \"\"\n",
    "            res[i]['p_tag'] = \"\"\n",
    "        continue\n",
    "        \n",
    "    if res[i]['Source_Lang'] != 'en':\n",
    "        try:\n",
    "            blob = TextBlob(res[i]['Source_PageTextOriginal'])\n",
    "            trans_text = str(blob.translate(from_lang=res[i]['Source_Lang'],to='en'))\n",
    "        except Exception as e:\n",
    "            print(\"Error found in Blob: \", e)\n",
    "            try:\n",
    "                trans_text = str(blob.translate(to='en'))\n",
    "                print('Solved')\n",
    "            except:\n",
    "                print('Unolved')\n",
    "                trans_text = res[i]['Source_PageTextOriginal']\n",
    "        res[i]['Source_PageTextEnglish'] = trans_text\n",
    "        time.sleep(randint(1,2))\n",
    "\n",
    "        try:\n",
    "            blob = TextBlob(res[i]['p_tag'])\n",
    "            trans_text = str(blob.translate(from_lang=res[i]['Source_Lang'],to='en'))\n",
    "        except Exception as e:\n",
    "            print(\"p_tag Error found in Blob: \", e)\n",
    "            try:\n",
    "                trans_text = str(blob.translate(to='en'))\n",
    "                print('Solved')\n",
    "            except:\n",
    "                print('Unolved')\n",
    "                trans_text = res[i]['p_tag']\n",
    "        res[i]['p_tag'] = trans_text\n",
    "        time.sleep(randint(1,2))\n",
    "        \n",
    "    else:\n",
    "        res[i]['Source_PageTextEnglish'] = res[i]['Source_PageTextOriginal']\n",
    "\n",
    "print(\"Translated Non-English Debunk page text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import string\n",
    "\n",
    "\n",
    "class MediaTypeOrg:\n",
    "    def __init__(self):\n",
    "        punct_chars = list(set(string.punctuation)-set(\"_\"))\n",
    "        punctuation = ''.join(punct_chars)\n",
    "        self.replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "        self.multi_word_dict = {'social media':'social_media'}\n",
    "\n",
    "        self.facebook = ['facebook', 'fb', 'faceboos', 'Facebok','facebookk']\n",
    "        self.twitter = ['twitter', 'tweets','tweet']\n",
    "        self.news = ['media', 'news', 'newspaper', 'newspapers', 'times', 'abcnews','CGTN']\n",
    "        self.whatsApp = ['whatsapp', 'wa', 'whatsap']\n",
    "        self.email = ['email']\n",
    "        self.social_media = ['social_media'] #######\n",
    "        self.youtube = ['youtube', 'youtuber']\n",
    "        self.blog = ['blog', 'bloggers', 'weibo', 'wechat', 'blogs', 'blogger']\n",
    "        self.instagram = ['instagram', 'ig']\n",
    "        self.tv = ['tv']\n",
    "        self.line = ['line']\n",
    "        self.chainMessage = ['chain', 'telegram']\n",
    "\n",
    "        self.type_dict={\n",
    "                'facebook': self.facebook,\n",
    "                'twitter': self.twitter,\n",
    "                'news': self.news,\n",
    "                'whatsApp': self.whatsApp,\n",
    "                'email': self.email,\n",
    "                'youtube': self.youtube,\n",
    "                'blog': self.blog,\n",
    "                'instagram': self.instagram,\n",
    "                'tv': self.tv,\n",
    "                'line': self.line,\n",
    "                'chain_message': self.chainMessage,\n",
    "                'social_media': self.social_media\n",
    "                }\n",
    "\n",
    "\n",
    "    def type_org(self, data):\n",
    "        addi_type_dict = {}\n",
    "        num_other = 0\n",
    "\n",
    "        for ifcn_link in data:\n",
    "            media_type = data[ifcn_link]\n",
    "            included_type_list = self.get_type(media_type)\n",
    "            if len(included_type_list) == 0:\n",
    "                num_other += 1\n",
    "                included_type_list.append('other')\n",
    "\n",
    "            addi_type_dict[ifcn_link] = included_type_list\n",
    "\n",
    "        return addi_type_dict\n",
    "\n",
    "    def _check_item_in_list(self, tokened, check_list):\n",
    "        for item in tokened:\n",
    "            if item in check_list:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _get_multiword(self, lower_case_string):\n",
    "        for item in self.multi_word_dict:\n",
    "            lower_case_string = re.sub(item, self.multi_word_dict[item], lower_case_string)\n",
    "        return lower_case_string\n",
    "\n",
    "\n",
    "\n",
    "    def get_type(self, raw_media_type):\n",
    "        lc_raw_mt = raw_media_type.lower()\n",
    "        lc_raw_mt = self._get_multiword(lc_raw_mt)\n",
    "        lc_raw_mt_re = self.replace.sub(' ', lc_raw_mt)\n",
    "\n",
    "        tokened = nltk.word_tokenize(lc_raw_mt_re)\n",
    "\n",
    "        included_type_list = []\n",
    "        for media_type in  self.type_dict:\n",
    "            check_list = self.type_dict[media_type]\n",
    "            type_included = self._check_item_in_list(tokened, check_list)\n",
    "            if type_included:\n",
    "                included_type_list.append(media_type)\n",
    "\n",
    "        return included_type_list\n",
    "\n",
    "\n",
    "            \n",
    "def merge_type(ori_raw_data, addi_type_dict):\n",
    "    passed = 0\n",
    "    new_data = []\n",
    "    for each_data in ori_raw_data:\n",
    "        try:\n",
    "            ifcn_link = each_data['Link']\n",
    "            each_data['Claim_Website'] = addi_type_dict[ifcn_link]\n",
    "            new_data.append(each_data)\n",
    "        except:\n",
    "            passed +=1\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "origin_link_dict = {}\n",
    "for i in range(len(res)):\n",
    "    origin_link_dict[res[i]['Link']]=res[i]['Origin']\n",
    "\n",
    "typeCheck = MediaTypeOrg()\n",
    "addi_type_dict = typeCheck.type_org(origin_link_dict)\n",
    "res = merge_type(res, addi_type_dict)\n",
    "\n",
    "punct_chars = list(set(string.punctuation)-set(\"_\"))\n",
    "punctuation = ''.join(punct_chars)\n",
    "pun_replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "for i in range(len(res)): \n",
    "    claim=res[i]['Origin']\n",
    "    claim = claim.lower()\n",
    "    claim = pun_replace.sub(' ', claim)\n",
    "    claim_tok = nltk.word_tokenize(claim)\n",
    "    res[i]['Claim_web_ori']=claim_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data as json\n",
    "\n",
    "import json\n",
    "with open('sample.json', 'w') as fo:\n",
    "    json.dump(res, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
